#!/usr/bin/env python3
"""
Test script for the LLM memory system.
Run this from the project root directory.
"""
import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.absolute()
sys.path.insert(0, str(project_root))

def test_memory_system():
    """Test the LLM memory system with sample data."""

    print("üß† Testing LLM Memory System...")

    try:
        from memory.simple_memory_workflow import LLMMemoryWorkflow
        print("‚úÖ Successfully imported LLMMemoryWorkflow")
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False

    try:
        # Create workflow
        workflow = LLMMemoryWorkflow()
        print("‚úÖ Successfully created workflow")

        # Check questions
        questions = workflow.get_questions()
        print(f"‚úÖ Loaded {len(questions)} questions")

        # Test with sample conversation
        print("\nüìù Testing sample conversation...")

        test_qa = [
            ("destination", "Where are you planning to travel?", "Santa Barbara, CA"),
            ("trip_purpose", "What's the main purpose of your trip?", "family vacation"),
            ("budget_preference", "What's your budget range per night?", "$200 per night"),
            ("amenities_features", "What hotel amenities are important?", "pool and breakfast"),
            ("stay_experience", "Describe your ideal hotel experience.", "family-friendly boutique hotel")
        ]

        for i, (q_id, q_text, answer) in enumerate(test_qa, 1):
            print(f"\n{i}. Testing question: {q_id}")
            print(f"   Answer: {answer}")

            try:
                validated, suggestions = workflow.validate_answer(q_id, q_text, answer)
                print(f"   ‚úÖ Validated: {validated}")
                if suggestions:
                    print(f"   üí° Suggestions: {len(suggestions)} offered")
                else:
                    print(f"   ‚úÖ No suggestions needed")
            except Exception as e:
                print(f"   ‚ùå Validation failed: {e}")
                return False

        # Test conversation processing
        print("\nüîÑ Processing complete conversation...")
        try:
            results = workflow.process_conversation()
            print("‚úÖ Conversation processed successfully")

            # Show insights if available
            insights = results.get("llm_insights", {})
            if insights and not insights.get("error"):
                print("\nüéØ LLM Insights:")
                for key, value in insights.items():
                    if value and value != "Not specified":
                        print(f"   {key}: {value}")
            else:
                print("‚ö†Ô∏è  LLM insights not generated (check Ollama connection)")

            print(f"‚úÖ Conversation consistent: {results.get('is_consistent', 'Unknown')}")

            if results.get("conversation_file"):
                print(f"‚úÖ Conversation saved to: {results['conversation_file']}")

        except Exception as e:
            print(f"‚ùå Conversation processing failed: {e}")
            return False

        print("\nüéâ All tests passed!")
        return True

    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        return False


def test_ollama_connection():
    """Test if Ollama is available."""
    print("\nüîå Testing Ollama connection...")

    try:
        from llm.ollama_client import get_ollama_client
        client = get_ollama_client()
        print("‚úÖ Ollama client created successfully")
        return True
    except Exception as e:
        print(f"‚ùå Ollama connection failed: {e}")
        print("   Make sure Ollama is running: ollama serve")
        return False


def main():
    """Main test function."""
    print("üöÄ Hotel Preference Memory System Test")
    print("=" * 50)

    # Test Ollama first
    ollama_ok = test_ollama_connection()

    # Test memory system
    memory_ok = test_memory_system()

    print("\n" + "=" * 50)
    if memory_ok:
        print("üéâ Memory system is working!")
        if not ollama_ok:
            print("‚ö†Ô∏è  Note: Some LLM features may not work without Ollama")
    else:
        print("‚ùå Memory system has issues")

    return memory_ok


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
